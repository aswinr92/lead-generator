# Integrated Workflow Guide

**Single Command. Complete Solution.**

Run `python main.py` and get everything done: scraping, cleaning, deduplication (including existing Google Sheets data), and export.

## ðŸŽ¯ Overview

The integrated workflow solves three critical problems:

1. **Only cleaned data goes to Google Sheets** - No raw, messy data
2. **No duplicate vendors across runs** - Deduplicates against existing Sheets data
3. **Single command execution** - No manual steps between scraping and export

## ðŸš€ Quick Start

### First Time Setup

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Setup Google Sheets credentials (one-time)
# Follow: docs/GOOGLE_SHEETS_QUICKSTART.md

# 3. Create your first sheet and run full workflow
python main.py --interactive
```

### Subsequent Runs

```bash
# Just run this - everything is automated!
python main.py
```

The system will:
1. âœ… Scrape new vendors from Google Maps
2. âœ… Download existing vendors from your Google Sheet
3. âœ… Merge and deduplicate everything together
4. âœ… Upload only unique, cleaned vendors back to Sheets
5. âœ… Keep your data quality high

## ðŸ“‹ Command Options

### Basic Commands

```bash
# Full workflow with existing sheet
python main.py --sheet-id YOUR_SHEET_ID

# Interactive mode (prompts for sheet ID)
python main.py --interactive

# Skip scraping (clean and export existing CSVs only)
python main.py --skip-scraping --sheet-id YOUR_SHEET_ID

# Skip export (scrape and clean only)
python main.py --skip-export

# Full workflow with auto-cleanup (deletes raw CSVs after success)
python main.py --sheet-id YOUR_SHEET_ID --auto-cleanup
```

### Advanced Options

```bash
# Custom config file
python main.py --config custom_config.yaml --sheet-id YOUR_SHEET_ID

# View help
python main.py --help
```

## ðŸ”„ How It Works

### The Complete Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: SCRAPE NEW VENDORS                                 â”‚
â”‚ - Google Maps search for configured cities/categories      â”‚
â”‚ - Extract vendor details                                   â”‚
â”‚ - Save to: output/vendors_YYYYMMDD_HHMMSS.csv (raw)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: CLEAN & MERGE RAW DATA                            â”‚
â”‚ - Find all raw CSV files                                   â”‚
â”‚ - Standardize fields (phone, address, names, etc.)        â”‚
â”‚ - Deduplicate within new data                             â”‚
â”‚ - Save to: output/vendors_cleaned_YYYYMMDD_HHMMSS.csv     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: DOWNLOAD EXISTING GOOGLE SHEETS DATA              â”‚
â”‚ - Connect to your Google Sheet                            â”‚
â”‚ - Download all existing vendors                           â”‚
â”‚ - Prepare for merge with new data                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: SMART DEDUPLICATION                               â”‚
â”‚ - Merge: Existing + New data                              â”‚
â”‚ - Strategy 1: Exact phone match                           â”‚
â”‚ - Strategy 2: Fuzzy name + address (85%/80%)              â”‚
â”‚ - Strategy 3: Name + city (90%)                           â”‚
â”‚ - Keep most complete data from all sources                â”‚
â”‚ - Save to: output/vendors_final_YYYYMMDD_HHMMSS.csv       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: EXPORT TO GOOGLE SHEETS                           â”‚
â”‚ - Upload final deduplicated data                          â”‚
â”‚ - Replace existing data (no accumulation)                 â”‚
â”‚ - Apply formatting (colors, filters, frozen rows)         â”‚
â”‚ - Generate summary statistics                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: CLEANUP (Optional)                                â”‚
â”‚ - Delete raw CSV files if --auto-cleanup enabled          â”‚
â”‚ - Keep cleaned and final CSVs for audit                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸŽ¯ Key Features

### 1. Smart Deduplication with Existing Data

**Problem:** Without this, every scraper run adds duplicates to your Sheet

**Solution:**
```bash
# Run 1:
python main.py --sheet-id ABC123
# Scrapes 50 vendors â†’ Uploads 50 to Sheet

# Run 2 (week later):
python main.py  # Uses saved sheet ID
# Scrapes 60 vendors (20 are duplicates)
# Downloads 50 existing from Sheet
# Deduplicates: 110 total â†’ 90 unique
# Uploads 90 to Sheet (replaces old data)

# Result: No duplicates accumulated!
```

### 2. Only Cleaned Data in Sheets

**Before integrated workflow:**
- Raw CSV â†’ Manual cleaning â†’ Manual deduplication â†’ Manual export
- Risk of uploading messy data

**With integrated workflow:**
- Automatic cleaning pipeline
- Only final, validated data goes to Sheets
- Quality scores ensure data completeness

### 3. Saved Sheet ID

First run:
```bash
python main.py --sheet-id YOUR_SHEET_ID
```

This saves the sheet ID to `config/sheet_id.txt`

All subsequent runs:
```bash
python main.py  # Automatically uses saved sheet ID
```

### 4. Flexible Workflows

```bash
# Daily scraping workflow
python main.py

# Data-only update (no new scraping)
python main.py --skip-scraping

# Scrape without export (testing)
python main.py --skip-export

# Clean house (delete raw files)
python main.py --auto-cleanup
```

## ðŸ“Š Example Scenarios

### Scenario 1: First Run

```bash
$ python main.py --interactive

ðŸŽ‰ WEDDING VENDOR SCRAPER - itsmy.wedding
ðŸš€ Integrated Workflow: Scrape â†’ Clean â†’ Deduplicate â†’ Export

ðŸ“ Google Sheet ID not found
Enter your Google Sheet ID (or press Enter to skip export): 1a2b3c4d5e6f

âš™ï¸  Workflow Configuration:
   Scraping:       âœ“ RUN
   Cleaning:       âœ“ RUN
   Deduplication:  âœ“ RUN
   Export:         âœ“ RUN
   Auto-cleanup:   âœ— NO
   Sheet ID:       1a2b3c4d5e6f

â“ Proceed with this configuration?
   Enter 'yes' to continue: yes

ðŸ“ STEP 1: SCRAPING VENDORS
âœ“ Scraped 50 vendors

ðŸ§¹ STEP 2: CLEANING & DEDUPLICATION
âœ“ Cleaned 50 â†’ 48 unique vendors

ðŸ”„ STEP 3: MERGE WITH EXISTING SHEETS DATA
ðŸ“¥ Downloading existing data...
   No existing data found
âœ… Using new data only

ðŸ“¤ STEP 4: EXPORT TO GOOGLE SHEETS
âœ… Exported 48 vendors to Google Sheets

ðŸ“Š WORKFLOW SUMMARY
ðŸ“ Scraping: 50 vendors
ðŸ§¹ Cleaning: 48 vendors
ðŸ”„ Merging: 0 existing + 48 new = 48 final
ðŸ“¤ Export: 48 vendors

âœ… WORKFLOW COMPLETE
```

### Scenario 2: Second Run (Week Later)

```bash
$ python main.py

ðŸŽ‰ WEDDING VENDOR SCRAPER - itsmy.wedding
ðŸ“‹ Using saved Sheet ID: 1a2b3c4d5e6f

ðŸ“ STEP 1: SCRAPING VENDORS
âœ“ Scraped 60 vendors

ðŸ§¹ STEP 2: CLEANING & DEDUPLICATION
âœ“ Cleaned 60 â†’ 55 unique vendors

ðŸ”„ STEP 3: MERGE WITH EXISTING SHEETS DATA
ðŸ“¥ Downloading existing data from Google Sheets...
   âœ“ Downloaded 48 existing records

ðŸ”„ Merging new data with existing data...
   Existing: 48 records
   New: 55 records
   Combined: 103 records

ðŸ” Deduplicating combined data...
   Found 20 duplicate groups
âœ… Deduplication complete: 103 â†’ 83 unique vendors

ðŸ“Š GOOGLE SHEETS MERGE REPORT
Existing vendors in Sheets: 48
Newly scraped vendors:      55
Total before dedup:         103
Duplicates removed:         20
New unique vendors:         35
Final vendor count:         83

ðŸ“¤ STEP 4: EXPORT TO GOOGLE SHEETS
âœ… Updated existing sheet with 83 vendors

ðŸ“Š WORKFLOW SUMMARY
ðŸ“ Scraping: 60 vendors
ðŸ§¹ Cleaning: 55 vendors
ðŸ”„ Merging: 48 existing + 35 new = 83 final
ðŸ“¤ Export: 83 vendors

âœ… WORKFLOW COMPLETE
```

### Scenario 3: Clean Existing Data Only

```bash
$ python main.py --skip-scraping

âš™ï¸  Workflow Configuration:
   Scraping:       â­ï¸  SKIP
   Cleaning:       âœ“ RUN

ðŸ§¹ STEP 2: CLEANING & DEDUPLICATION
ðŸ“‚ Found 3 raw CSV files:
   - vendors_20260208_221125.csv
   - vendors_20260208_235209.csv
   - vendors_20260209_120000.csv

âœ“ Merged 150 records â†’ 110 unique vendors

ðŸ”„ STEP 3: MERGE WITH EXISTING SHEETS DATA
âœ“ Merged with 83 existing â†’ 140 final

ðŸ“¤ STEP 4: EXPORT TO GOOGLE SHEETS
âœ… Updated sheet with 140 vendors

âœ… WORKFLOW COMPLETE
```

## ðŸ›¡ï¸ Data Safety

### What Gets Deleted?

**With `--auto-cleanup`:**
- âœ… **Deleted:** Raw CSV files (`vendors_YYYYMMDD_HHMMSS.csv`)
- âœ… **Kept:** Cleaned CSVs (`vendors_cleaned_*.csv`)
- âœ… **Kept:** Final CSVs (`vendors_final_*.csv`)
- âœ… **Kept:** Reports (`cleaning_report_*.txt`)

**Without `--auto-cleanup` (default):**
- âœ… **Kept:** Everything

### Backup Strategy

```bash
# 1. Raw data is in Google Sheets (always accessible)
# 2. Cleaned CSVs are always kept
# 3. Final CSVs show exactly what was uploaded

# If you need to restore:
# Download from Google Sheets â†’ CSV â†’ Re-import
```

## âš™ï¸ Configuration

### Sheet ID Storage

The sheet ID is saved to `config/sheet_id.txt`:

```
1a2b3c4d5e6f7g8h9i0j
```

You can:
- Edit this file manually
- Override with `--sheet-id` argument
- Remove file to start fresh

### Scraping Configuration

Edit `config/config.yaml`:

```yaml
cities:
  - Trivandrum
  - Kochi
  - Thrissur

categories:
  - wedding caterers
  - wedding photographers
  - wedding decorators

scraping:
  max_results_per_search: 50
  rate_limit_delay: 3
  headless: false
```

## ðŸ› Troubleshooting

### Issue: "No sheet ID found"

```bash
# Solution 1: Provide sheet ID
python main.py --sheet-id YOUR_SHEET_ID

# Solution 2: Use interactive mode
python main.py --interactive

# Solution 3: Create config/sheet_id.txt manually
echo "YOUR_SHEET_ID" > config/sheet_id.txt
```

### Issue: "Could not connect to Google Sheets"

```bash
# Check credentials exist
ls config/google_credentials.json

# If missing, follow setup guide
cat docs/GOOGLE_SHEETS_QUICKSTART.md

# Test connection
python verify_setup.py
```

### Issue: "Too many duplicates kept/removed"

```bash
# Adjust thresholds by editing:
# processors/deduplicator.py

# Or run standalone cleaning with custom thresholds:
python clean_data.py --name-threshold 90 --address-threshold 85
```

### Issue: "Scraper getting blocked"

```bash
# Increase delays in config/config.yaml
scraping:
  rate_limit_delay: 5  # Increase this

# Or run in non-headless mode
scraping:
  headless: false
```

## ðŸ“ˆ Performance

**Typical Run Times:**
- Scraping: ~5-10 min (50 vendors)
- Cleaning: ~5-10 sec (100 records)
- Deduplication: ~10-20 sec (200 records)
- Sheets Export: ~5-10 sec
- **Total: ~6-12 minutes**

**Scalability:**
- Can handle 10,000+ vendors
- Deduplication scales linearly
- Google Sheets has 10M cells limit

## ðŸ’¡ Best Practices

### 1. Regular Scraping

```bash
# Weekly workflow
python main.py

# This ensures:
# - Fresh vendor data
# - Updated ratings/reviews
# - New vendors discovered
```

### 2. Review Reports

```bash
# After each run, check:
cat output/cleaning_report_*.txt

# Look for:
# - Data quality scores
# - Missing fields
# - Duplicate patterns
```

### 3. Backup Sheet ID

```bash
# Keep a copy of your sheet ID
cat config/sheet_id.txt

# Or bookmark your sheet URL
```

### 4. Monitor Data Quality

```python
# Check quality trends
import pandas as pd

df = pd.read_csv('output/vendors_final_*.csv')
print(f"Avg quality: {df['quality_score'].mean():.1f}")
print(f"High quality: {(df['quality_score'] >= 80).sum()}")
```

### 5. Test Before Production

```bash
# Test scraping only
python main.py --skip-export

# Review data
cat output/vendors_cleaned_*.csv | head -20

# Then export
python main.py --skip-scraping --sheet-id YOUR_SHEET_ID
```

## ðŸ”„ Comparison: Old vs New

### Old Workflow (3 Commands)

```bash
# Step 1: Scrape
python main_scraper_only.py
# Output: vendors_20260209.csv (raw, messy)

# Step 2: Clean manually
python clean_data.py --input output/vendors_20260209.csv
# Output: vendors_cleaned_20260209.csv

# Step 3: Export manually
python export_to_sheets.py --input output/vendors_cleaned_20260209.csv
# Problem: Duplicates accumulate in Sheets!

# Step 4: Manual deduplication in Sheets (ugh!)
```

**Problems:**
- âŒ Manual steps between each phase
- âŒ Easy to forget cleaning
- âŒ Duplicates accumulate in Sheets
- âŒ Need to track which files to use

### New Workflow (1 Command)

```bash
# Just this!
python main.py
```

**Benefits:**
- âœ… Fully automated pipeline
- âœ… Always cleans before export
- âœ… Deduplicates against existing Sheets data
- âœ… No duplicate accumulation
- âœ… Single source of truth

## ðŸ“š Related Documentation

- **Quick Start:** `QUICKSTART.md`
- **Data Cleaning:** `DATA_CLEANING_GUIDE.md`
- **Google Sheets:** `GOOGLE_SHEETS_QUICKSTART.md`
- **System Overview:** `SYSTEM_OVERVIEW.md`

## ðŸŽ“ Advanced Usage

### Custom Workflow Scripts

```python
from integrated_workflow import run_integrated_workflow

# Run with custom parameters
stats = run_integrated_workflow(
    skip_scraping=False,
    skip_export=False,
    auto_cleanup=True,
    sheet_id='YOUR_SHEET_ID'
)

print(f"Final vendors: {stats['merging']['final_count']}")
```

### Scheduled Runs

```bash
# Linux/Mac crontab
0 0 * * 0 cd /path/to/scraper && python main.py >> logs/weekly.log 2>&1

# Windows Task Scheduler
# Create task to run: python main.py
# Schedule: Weekly, Sunday 12:00 AM
```

---

**Last Updated:** 2026-02-09
**Version:** 2.0.0 (Integrated Workflow)
